<!doctype html><html lang=en><head><link crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV rel=stylesheet><script crossorigin=anonymous defer integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js></script><script onload="renderMathInElement(document.body, {delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\(', right: '\\)', display: false},
        {left: '\\[', right: '\\]', display: true}
    ]});" crossorigin=anonymous defer integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js></script><link href=https://riasanovsky.me/katex.css rel=stylesheet></head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><title> A Monte Carlo Tree Search for Reward-Friendly Optimization Problems </title><link href=self.jpg rel=icon type=image/png><link href="https://riasanovsky.me/ atom.xml" title="Dr. Alex Riasanovsky" rel=alternate type=application/atom+xml><link href=https://riasanovsky.me/main.css media=screen rel=stylesheet><meta name=description><meta name=description><meta content="index, nofollow" name=robots><meta content="Dr. Alex Riasanovsky" property=og:title><meta content=article property=og:type><meta content=self.jpg property=og:image><meta content=self.jpg name=twitter:card><meta content=https://riasanovsky.me/posts/2023-09-mcts/ property=og:url><meta property=og:description><meta content="Dr. Alex Riasanovsky" property=og:site_name><meta content="default-src 'self' ws://127.0.0.1:1024/; img-src 'self' https://*; script-src 'self'; style-src 'self'; font-src 'self'" http-equiv=Content-Security-Policy><body><header><div class=navbar><div class="nav-title nav-navs"><a class="nav-links home-title" href=https://riasanovsky.me>Dr. Alex Riasanovsky</a></div><nav class="nav-title nav-navs"><a class=nav-links href=/projects> <img alt=Projects height=15 src=/menu_icon/projects.png width=15> Projects</a><a class=nav-links href=/about> <img alt=About height=15 src=/menu_icon/self.jpg width=15> About</a></nav><nav class="socials nav-navs"><a class="nav-links social" rel="noopener noreferrer" href=https://github.com/ariasanovsky/ target=_blank> <img alt=github src=/social_icons/github.svg title=github> </a><a class="nav-links social" rel="noopener noreferrer" href=https://twitter.com/AlexMath0 target=_blank> <img alt=twitter src=/social_icons/twitter.svg title=twitter> </a><label class=theme-switcher for=themeswitch><div class=background></div> <input id=themeswitch type=checkbox> <div class=switch><img alt="theme switch to dark" class=moon src=/menu_icon/moon.png><img alt="theme switch to light" class=sun src=/menu_icon/sun.png></div></label></nav></div></header><div class=content><main><article><div class=title><h2>A Monte Carlo Tree Search for Reward-Friendly Optimization Problems</h2><div class=meta>Posted on <time>2023-09-30</time><div class=post-tags><nav class="nav tags">üè∑: <a href=https://riasanovsky.me/tags/mcts/>mcts</a> ¬† <a href=https://riasanovsky.me/tags/rl/>RL</a> ¬† <a href=https://riasanovsky.me/tags/ml/>ML</a> ¬† <a href=https://riasanovsky.me/tags/deep-learning/>deep learning</a> ¬† <a href=https://riasanovsky.me/tags/alpha-zero/>alpha zero</a> ¬† <a href=https://riasanovsky.me/tags/rust/>rust</a> ¬†</nav></div> ||<span> 13 minute read</span></div></div><h1>Table of Contents</h1><ul><li><a href=https://riasanovsky.me/posts/2023-09-mcts/#background>Background</a> <ul><li><a href=https://riasanovsky.me/posts/2023-09-mcts/#chess>Chess</a><li><a href=https://riasanovsky.me/posts/2023-09-mcts/#space-invaders>Space Invaders</a></ul><li><a href=https://riasanovsky.me/posts/2023-09-mcts/#policies>Policies</a><li><a href=https://riasanovsky.me/posts/2023-09-mcts/#monte-carlo-tree-search>Monte Carlo Tree Search</a><li><a href=https://riasanovsky.me/posts/2023-09-mcts/#alphazero>AlphaZero</a> <ul><li><a href=https://riasanovsky.me/posts/2023-09-mcts/#formulation>Formulation</a><li><a href=https://riasanovsky.me/posts/2023-09-mcts/#learning-loop>Learning loop</a></ul><li><a href=https://riasanovsky.me/posts/2023-09-mcts/#irtree>IRTree</a> <ul><li><a href=https://riasanovsky.me/posts/2023-09-mcts/#assumptions>Assumptions</a><li><a href=https://riasanovsky.me/posts/2023-09-mcts/#transitions>Transitions</a><li><a href=https://riasanovsky.me/posts/2023-09-mcts/#model>Model</a><li><a href=https://riasanovsky.me/posts/2023-09-mcts/#estimates>Estimates</a></ul></ul><section class=body><p>This post is the whitepaper for a modification I am making to the AlphaZero algorithm for a family of optimization problems I am interested in. We expose the topic and end with a technical breakdown of the key data structure used.<h2 id=background>Background</h2><p>Pick your favorite state space $\mathcal{S}$ and endow each $s\in\mathcal{S}$ with a set $\mathcal{A}(s)$ of actions. For each \(a\in\mathcal{A}(s)\), then $as\in \mathcal{S}$ is formed by taking $a$ from $s$. The value of $as$ may be deterministic or stochastic (e.g., in the context of single-player games) or policy-dependent (e.g., in the context of multiplayer games). If $\mathcal{A}(s) = \emptyset$, then $s$ is <em>terminal</em>. Additionally, for each terminal $s$, we have a valuation $v(s)$ which we seek to optimize.<h3 id=chess>Chess</h3><p>In this classic example, we play as White against an opponent playing with some fixed policy. Here,<ul><li>$\mathcal{S}$: All possible chess board positions with White to play. <ul><li>This includes piece positions, the states of castling, and information about $3$-fold repetition and the $50$-move rules.</ul><li>$\mathcal{A}$: All legal moves for White to play.<li>$as$: When White takes move $a$, a board $s'$ with Black to play is formed; Black plays according to their policy, producing a board $as$ with White to play. <ul><li>When Black has no move, instead $as$ is the board $s'$ with a decoration that White has won or forced a stalemate.<li>This requires a small modification to the defintion of $\mathcal{S}$.</ul><li>$v(s)$: If the game is over, a score of $+1,$ $0,$ or $-1$ indicates the winning player.</ul><p>If White and Black play with respect to stochastic policies $\pi_W, \pi_B$, then we are encouraged to extend $v$ from terminal states to $$v_{\pi_W, \pi_B}(s) := \mathbb{E}[v(s') : \text{from }s, \text{terminal state }s'\text{ is reached when Black and White follow }\pi_W\text{ and }\pi_B,\text{respectively}]$$<h3 id=space-invaders>Space Invaders</h3><p>In this example:<ul><li>$\mathcal{S}$: all possible single-frame game states in the game <em>Space Invaders</em> <ul><li>this includes positions and velocities of the player and all asteriods<li>it also includes score and other internal game states</ul><li>$\mathcal{A}(s)$: combinations of button inputs legal in <em>Space Invaders</em><li>$as$: the result of playing a combination $a$ of buttons for a frame from state $s$<li>$v(s)$: if the game is over, the player's current score.</ul><p>While it may be tempting to extend $v$ to $\mathcal{S}$ with the <em>current score</em> function, we follow the previous convention:<p>$$v_{\pi}(s) := \mathbb{E}[v(s') : \text{from }s, \text{terminal state }s'\text{ is reached when the player follows }\pi]$$<h2 id=policies>Policies</h2><p>A policy maps each nonterminal state $s$ to a probability distribution over $\mathcal{A}(s)$. Some policies are better than others, as determined by component-wise comparison. That is, we may say that $\pi\leq \pi'$ if for all states $s$, $v_\pi(s)\leq v_{\pi'}(s)$. Finding optimal policies is intractible, even when $\mathcal{S}$ and $\mathcal{A}(s)$ are finite and state transitions are deterministic.<p>We are forced to make tradeoffs when defining our policy $\pi$:<ul><li><strong>implementation</strong>: how to store $\pi$ so it can quickly evaluate states, but also be fine-tuned with new data,<li><strong>exploitation</strong>: how much to trust that $\pi$ will lead us to the best terminal states if we follow it greedily, and<li><strong>exploration</strong>: how much to distrust $\pi$, taking less preferred actions in order to gather data counteracting our bias.</ul><h2 id=monte-carlo-tree-search><a href=https://en.wikipedia.org/wiki/Monte_Carlo_tree_search>Monte Carlo Tree Search</a></h2><p>When building a policy $\pi$ from nothing, we conduct a tree search as following:<ul><li>$s_{\text{root}}$: select a root in $S$ and build a tree<li><strong>transitions</strong>: Explore from the root, recording the transitions $(a_1, s_1), \cdots, (a_t, s_t)$. <ul><li>For all $i=1..t$, $a_i\in\mathcal{A}(s_{i-1})$ is the action taken from $s_{i-1}$, which produced $s_i$.<li>$v$: $s_t$ is terminal with value $v = v(s_t)$.</ul><li><strong>biases</strong>: When selecting an action $a\in\mathcal{A}(s)$, we bias ourselves to: <ul><li><strong>exploration</strong>: Infrequently visited state-action pairs are preferable. <ul><li>Log the values $n(s)$, the number of actions taken from $s$ during the tree's lifetime.<li>Log also the values $n(s, a)$, the number of visits to $(s, a)$. <ul><li>If practical, we may have access to $n(sa)$ and do not need to define $n(s, a)$.</ul></ul><li><strong>score</strong>: Actions which previously led to a high score are preferable. <ul><li>Log $w(s, a)$, the sum of all terminal scores over all explored paths with the transition $(s, a)$. <ul><li>If practical, we may instead log $w(s)$ defined similarly and use $w(sa)$ to inform us about $a$.<li>This complicates how the average of $w$ is calculated.</ul><li>We are biased to take $a$ from $s$ if $w(s, a) / n(s, a)$ is high.</ul></ul></ul><p>This begs several imporatant questions:<ul><li><strong>prior distributions</strong>: How should pick actions from unvisited nodes? <ul><li>When the tree is initialized, all values of $n(s), n(s, a),$ and $w(s, a)$ are $0$.<li><em>A priori</em>, we have no bias to suggest what action to take.<li>In most games, uniform distrubtions are terrible guesses.<li>We encounter this problem each time a new (nonterminal) node is added to the tree.</ul><li><strong>bias terms</strong>: How should we pick actions from visited nodes? <ul><li>Even when $s$ has been visited, what is biases us towards one action over another? For example, $$\dfrac{w(s, a)}{n(s, a)} + c\cdot\dfrac{\sqrt{n(s)}}{n(s, a)}.$$ <ul><li>This choice is extremely arbitrary. Many others may perform better.</ul><li>How practically can we store information about every visited action from every visited state?<li>How fast will we converge to an optimal policy, if at all?</ul><li><strong>learning</strong>: Are we expected to trust our observations? <ul><li>Is the policy identical to the entire tree?<li>How practically can we use it in future simulations?</ul></ul><h2 id=alphazero>AlphaZero</h2><h3 id=formulation>Formulation</h3><p>In this paper, the DeepMind team made crucial modifications to the classical MCTS search. First, they use an artificial neural network to define a model $$f_\theta: \mathcal{S}\to\mathbb{R}^{|\mathcal{A}| + 1}.$$<ul><li>By assumption, there exists a finite set $\mathcal{A}$ such that for all $s\in\mathcal{S}$, $\mathcal{A}(s)\subseteq\mathcal{A}$.<li>Here, $\theta$ is a high-dimensional real-valued vector of weights and biases for the underlying artificial neural network.</ul><p>For each $s\in\mathcal{S}$, we interpret $f_\theta(s)$ as the direct sum of $p_\theta(s, \cdot)$ and $v_\theta(s)$. Here:<ul><li>$p_\theta(s,\cdot)\in\mathbb{R}^{|\mathcal{A}|}$ may be interpreted as a probability distribution over $\mathcal{A}$. <ul><li>The actions in $\mathcal{A}\setminus\mathcal{A}(s)$ are ignored.<li>We may choose to normalize with the <a href=https://en.wikipedia.org/wiki/Softmax_function>softmax</a> function or a simple average.</ul><li>$v_\theta(s)\in\mathbb{R}$: a prediction for the terminal value of $v$ when following a policy defined as a function of $\theta$.</ul><p><strong>Note</strong>: In fact, $p_\theta(s, \cdot)$ is also a prediction. The model $f_\theta$ estimates the probability distribution of our agent, whose policy is defined as a function of $f_\theta$!<h3 id=learning-loop>Learning loop</h3><p>Rather than store large swaths of information in a Monte Carlo tree, our learning loop is as follows:<ul><li><strong>plant many trees</strong>: Create a batch of search trees to explore in parallel. <ul><li>Select roots with a bias towards high values of $v_\theta$.<li>Avoid batches of roots that are "similar" to each other. Perhaps $\mathcal{S}$ has a distance metric.</ul><li><strong>trust the model</strong>: The model may be used to enhance the search algorithm.<li>$p_\theta(s, a)$: Use this value to adjust the bias towards taking action $a$ from $s$. <ul><li>Dirichlet noise is added to encourage exploration.</ul><li>$v_\theta(s)$: When we encounter $s$, and is not terminal and unvisited, add $s$ to the tree and use $v_\theta(s)$. <ul><li>Since $s$ is not terminal, we may need to take many actions to reach a terminal state.<li>Trust the model's prediction will improve over time.<li>If $s$ is visited later, we will play moves from $a$.</ul><li><strong>chopped trees</strong>: Suppose a tree is rooted at $s_0$. <ul><li>After a few thousands simulations, we have explored a few thousand paths.<li>Many of those pathes ended by adding a new node to the tree and using $v_\theta$ to estimate the ending value.<li>Many ended with a terminal state.<li>Inside the tree, we have recorded $n(s_0)$ and $n(s_0, a)$ for all $a\in\mathcal{A}(s_0)$.<li>We also have recorded $w(s_0)$ and $w(s_0, a)$ for all $a\in\mathcal{A}(s_0)$.<li>We emit $p_{\text{obs}} = (n(s_0, a)/n(s_0))_{a\in\mathcal{A}}$.<li>We also emit $v_{\text{obs}} = w(s_0)\cdot/n(s_0)$.</ul><li><strong>model updates</strong>: With a batch of observations, we adjust the parameters $\theta$ in our model to decrease a loss function. <ul><li>Our batch consists of: <ul><li><strong>roots</strong>: $(s_i)_{i=1..B}$,<li><strong>observed values</strong>: $(v_{\text{obs, i}})_{i=1..B}$, and<li><strong>observed probabilities</strong>: $(p_{\text{obs, i}})_{i=1..B}$.</ul><li>Loss includes terms such as: <ul><li><strong>mis-estimated values</strong>: $(v_\theta(s_i) - v_{\text{obs, i}})^2$<li><strong>cross-entropy</strong>: $H(p, p) = -\sum_a p_{\text{obs, i}}(a)\cdot\log(p_\theta(s_i, a))$<li><strong>overfitting</strong>: $|\theta|_2^2$. (a.k.a. <em>regularization</em>)</ul></ul></ul><h2 id=irtree><code>IRTree</code></h2><p>This is my tentative name for a modification of AlphaZero to the context of mathematical optimization. Here:<ul><li><code>I(nitial)</code>: The value of $c(s)$ is only calculated directly for root states. All remaining values are computed dynamic with the reward function.<li><code>R(eward)</code>: When a state $s$ is added to the tree, we immediately record the reward function $r(s, \cdot)$ into memory.</ul><h3 id=assumptions>Assumptions</h3><ul><li>There exists a cost function $c:\mathcal{S}\to\mathbb{R}$ to minimize.<li>For all $s\in\mathcal{S}$, the reward function $r(s, \cdot): \mathcal{A}(s)\to\mathbb{R}$ is convenient to compute as a function of $s$. <ul><li>Here, for all $a\in\mathcal{A}(s)$, $r(s, a) = c(s) - c(as)$.</ul></ul><h3 id=transitions>Transitions</h3><p>Since we have a cheap reward function, we instead update the tree with improvement in the cost function <em>after</em> visiting $(s, a)$. Suppose $s_0\in\mathcal{S}$ and $\ell\in\mathbb{N}$. Suppose furthermore that $t = (t_i)_{i=1..\ell}\in(\mathcal{S}\times\mathbb{R}\times\mathcal{A})^\ell$ such that for all $i=1..\ell$:<ul><li>$t_i = (a_i, r_i, s_i)\in\mathcal{A}\times\mathbb{R}\times\mathcal{S}$,<li>$a_i\in\mathcal{A}(s_{i-1})$ and $r_i = r(s_{i-1}, a_i)$, and<li>$s_i = a_is_{i-1}$. Then we say that $t$ are the <em>transitions (of a path of length $\ell$ corresponding to $c$) from $s_0$</em>. Then we define the <em>max gains</em> $g^\ast = g_t^\ast = (g_i^\ast)_{i=0..t}$ (of $c$, from $s_0$, along $t$) as follows:<li>$g_t^\ast = 0$, and<li>$g_i^\ast = \max(0, r_{i+1} + g_{i+1}^\ast)$ for all $i = 0..t-1$. Note that for all $i = 0..t$, $$g_i^\ast = \max_{j = i..t}(r_{i+1} +\cdots+r_j).$$ Additionally, if $j^\ast$ attains the maximum with respect to $i$, then $j^\ast$ also attains $$\min_{j=i..t}c(s_j).$$ Finally, suppose $j$ attains the above maximum (or maxima) with respect to $i = 0$. Then we say that <em>$t$ observes an improvement in $c$ of <em>$g_0^\ast$</em> from $s_0$ (at $s_j$)</em>.</ul><p>Finally, suppose we have a model (with parameters $\theta$) which defines a prediction function $g_\theta^\ast:\mathcal{S}\to\mathbb{R}$. Then as above, we define the <em>predicted max gains</em> $h_{\theta}^\ast = h_{\theta, t}^{\ast} = (h_{i}^{\ast})_{i=0..t}$ (of $c$, from $s_0$, along $t$, with respect to $\theta$) as follows:<ul><li>$h_t^\ast = \max 0, g_\theta^\ast$, and<li>$h_i^\ast = \max(0, r_{i+1} + h_{i+1}^\ast)$ for all $i = 0..t-1$.</ul><p>For convenience, we extend $r = (r_i)_{i=1..t}$ to<p>$$r' = (r_i')_{i=1..t+1}$$<p>by letting $r_i' := r_i$ for all $i = 1..t$ and $r_{t+1}' := g_\theta^\ast(s_t)$.<p>As before, we note that for all $i = 0..t$,<p>$$h_i^\ast = \max_{j=i..t+1}(r_{i+1}'+\cdots+r_j').$$<p>For a final observation, let<p>$$c' := (c_i')_{i=0..t+1}$$<p>where $c_i' := c(s_i)$ for all $i = 0..t$ and $c_{t+1}' := c(s_t) + g_\theta^\ast(s_t)$. Then the maxima of $h_i^\ast$ and the minima of $c_i'$ are identical.<h3 id=model>Model</h3><p>We model with prediction function $$f_\theta(s) = \left(\begin{array}{r} p_\theta(s, \cdot)\ g_\theta(s)\ g_\theta^\ast(s)\ \tilde{g}_\theta^\ast(s) \end{array}\right)$$<p>Here, we predict:<ul><li>$g_\theta(s)$: the expected value of $c(s) - c(s')$ if a search starting with $s$ ends at terminal node $s'$.<li>$g_\theta^\ast(s)$: the maximum value of $c(s) - c(s')$ over all paths explored during the search. <ul><li>The number of paths explored is fixed.</ul><li>$\tilde{g}_\theta^\ast$: the expected maximum value of $c(s) - c(s')$ if a search starts with $s$ and passes through $s'$.</ul><p>We iteratively search and modify a tree $m$ rooted at some $s_{\text{root}}\in\mathcal{S}$. Each search in $m$ begins at $s_0 = s_{\text{root}}$, corresponds to a sequence $t = (t_i)_{i=1..\ell}$ of transitions as defined above.<p>Inside $m$, we identify states by a canonical path from $s_{\text{root}}$. Given the decomposition of each reward function as $$r(s, \cdot) = f(s, \cdot) + k(s, \cdot),$$ we store for each visited $s\in\mathcal{S}$ the following unchanging information inside $m$, in some implicit form:<ul><li><code>(mut)</code> $n(s)\in\mathbb{N}$, the frequency of $s$<li><code>(mut)</code> $\tilde{r}(s, \cdot): \mathcal{A}(s)\to\mathbb{R}$, our best estimate for $r(s, \cdot)$ <ul><li>initially $f(s, \cdot)$, and $\tilde{r}(s, a)$ is replaced by $f(s, a) + k(s, a)$ when $(s, a)$ is visited</ul><li>$p_\theta(s, \cdot): \mathcal{A}(s)\to\mathbb{R}$ Additionally, for each $a\in\mathcal{A}(s)$ taken from $s$ during the any search through $m$, we store:<li><code>(mut)</code> $n(s, a)\in\mathbb{N}$, the frequency of $(s, a)$<li><code>(mut)</code> $\gamma(s, a)$ described below. Here, $\gamma(s, a)$ is the total predicted max gains from $sa$ over all searches where $a$ is taken from $s$. In other words, we increment $\gamma(s, a)$ by $\tilde{g}_j^\ast$ in the following circumstance:<li>a search in $m$ corresponds to a sequence $t = (t_i)_{i=1..\ell}$<li>$t_j = (a, r_j, s_j)$ and $s_{j-1} = s$.</ul><h3 id=estimates>Estimates</h3><p>With tree $m$ rooted at $s_{\text{root}}$, we search $m$ for states which minimize $c$ . For our policy, we choose the upper estimate function defined by $$u(s, a) := \tilde{r}(s, a) + \overline{\gamma}(s, a) + C\cdot p(s, a)\cdot \dfrac{\sqrt{n(s)}}{1+n(s, a)}$$ where $\overline{\gamma}(s, a) = 0$ if $(s, a)$ is unvisited (i.e., $n(s, a) = 0$) and otherwise, $\overline{\gamma}(s, a) = \gamma(s, a) / n(s, a)$.</section></article></main></div><footer><section><nav><span classname=desktop-only>Made by <a rel="noopener noreferrer" href=https://syedzayyan.com/ target=_blank>SZM</a> and powered by <a rel="noopener noreferrer" href=https://getzola.org/ target=_blank>Zola</a></span></nav></section><script src=https://riasanovsky.me/js/main.js></script></footer>